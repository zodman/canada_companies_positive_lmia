init:
    wget "https://open.canada.ca/data/api/action/package_show?id=90fed587-1364-4f33-a9ee-208181dc0b97" --no-hsts -c -O out.json
    rows convert "https://www.canada.ca/en/immigration-refugees-citizenship/services/immigrate-canada/express-entry/eligibility/find-national-occupation-code.html" noc.csv

start:
    rm -f  *.csv
    declare -a FILES=`cat out.json  | jq -r ".result.resources[].url" | grep -v "/Positive_Employers_EN.csv" | grep EN` 
    for f in ${FILES}; do
        red " "
        red  " Download $f"
        wget -nc --no-hsts --show-progress $f
    done
    for f  in *.xlsx; do
        xlsx2csv $f > "${f%.xlsx}.csv"
    done
    red " "
    red "Remove shit"
    sed -i.orig.orig 1d *.csv
    sed -i.orig.orig 1d 2015_Positive_Employers_EN.csv

fillempty:
    set -e
    red "Fill empty columns"

    for f in *EN.csv; do
        red "Proccessing $f"
        python3 fillempty.py $f
    done

concat:
    declare -a FILES=`find . -iname '*.processed.csv' | xargs`
    red "merging files"
    rows csv-merge ${FILES} all.csv
    red "obtain schema"
    rows schema all.csv


convert:
    rm -rf *.db
    red "Finish step, generating db files"
    rows csv-to-sqlite all.csv all.db 
    sqlite-utils enable-fts --fts4 all.db all employer occupations address  phase  province_territory  stream
    rows csv-to-sqlite global_stream.csv global_stream.db
    rows csv-to-sqlite reduced_global_stream.csv reduced_global_stream.db
#    rows csv-to-sqlite reduced_global_stream.csv.loc.csv reduced_global_stream.csv.loc.db
#   rows csv-to-sqlite reduced_global_stream.csv.joburl.csv reduced_global_stream.csv.joburl.db

replacestr:
    red "Replace heads"
    sed -i.orig s/occupations_under_noc_2011/occupations/g *.processed.csv
    sed -i.orig s/approved_positions/positions_approved/g *.processed.csv
    sed -i.orig s/employer_name/employer/g *.processed.csv
    red "Replace head ocupations"
    sed -i.orig s/occupaitons_under_noc_2012/occupations/g *.processed.csv
    sed -i.orig s/occupaitons_under_noc_2011/occupations/g *.processed.csv
    sed -i.orig s/,occupation,/,occupations,/g *.processed.csv
    red "Replace head stream"
    sed -i.orig s/program_stream/stream/g *.processed.csv

globalstream:
    SQL="select * from table1 where stream like '%Global%'"
    rows query "${SQL}" all.csv --output global_stream.csv --samples 0

reduce:
    SQL=" select * from table1 where"
    SQL+=" ( lower(address) not like '%toronto%' and "
    SQL+=" lower(address) not like '%qu%bec%' and "
    SQL+=" lower(address) not like '%montr%al%' and "
    SQL+=" lower(address) not like '%vancouver%' ) and "
    SQL+=" ( lower(occupations) like '%comp%' or"
    SQL+=" lower(occupations) like '%soft%' )"
	echo ${SQL}
    rows query "${SQL}" global_stream.csv --output reduced_global_stream.csv
    rows query "select address,occupations from table1" reduced_global_stream.csv
fil:
    rows query "select distinct(employer) as e from table1 order by e desc " reduced_global_stream.csv  --output filtered.csv
    sed -i.orig s/Inc.//g filtered.csv
    sed -i.orig s/inc.//g filtered.csv
    sed -i.orig s/\\.//g filtered.csv
    sed -i.orig s/Ltd//g filtered.csv
    sed -i.orig s/\\\"//g filtered.csv
    sed -i.orig s/Technology//g filtered.csv
    sed -i.orig s/Technologie//g filtered.csv

all: fillempty replacestr concat globalstream  reduce fil convert
complete: clean init start all

clean:
    rm -rf *.csv
    rm -rf *.json
    rm -rf *.orig
    rm -rf *.xlsx


upload: @interactive
    rsync --progress -v *.db *.yml python3.ninja:apps/canada/
deploy: upload @interactive
    ssh zodman@python3.ninja sudo supervisorctl restart canada
